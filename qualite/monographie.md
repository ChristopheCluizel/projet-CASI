Big data and tweet streaming (Monographie)
===

## Partie A

### A0


### A1

* Tweet
* Spark streaming
* Parallélisation
* Visualisation
* Temps réel
* Profil utilisateur
* Data mining
* Big data
* Scalabilité
* Données


### A2

* [Spark streaming](http://spark.apache.org/streaming/)
* [ElasticSearch](https://www.elastic.co/products/elasticsearch)
* [Kibana](https://www.elastic.co/products/kibana)


### A3

* [Real-Time Analytics: Techniques to Analyze and Visualize Streaming Data](http://www.amazon.fr/Real-Time-Analytics-Techniques-Visualize-Streaming/dp/1118837916/ref=sr_1_7?s=english-books&ie=UTF8&qid=1446301986&sr=1-7&keywords=streaming). Chapitre 1 : « Introduction to Streaming data »
* [Learning Spark: Lightning-Fast Big Data Analysis](http://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis/dp/1449358624/ref=sr_1_1?ie=UTF8&qid=1446301706&sr=8-1&keywords=apache+spark). Chapitre 10 : « Spark Streaming »
* [Learning Real-time Processing with Spark Streaming](http://www.amazon.fr/Learning-Real-time-Processing-Spark-Streaming-ebook/dp/B015Q7I3NM/ref=sr_1_2?s=english-books&ie=UTF8&qid=1446301986&sr=1-2&keywords=streaming). Chapitre 2 : « Architecture and Components of Spark and Spark Streaming »

### A4

Apache

La fondation Apache est une organisation à but non lucratif dont l'objectif est de développer des logiciels open-source. Le logiciel le plus connu est sans aucun doute le serveur web Apache. Tout projet open-source sous license Apache 2 peut devenir un projet de la fondation Apache après une période d'incubation.
De nombreux logiciels de traitement de données sont soutenus par la fondation dont Apache Hadoop, Apache Spark, Apache Lucene ou encore Apache Kafka. Les frameworks de calcul Hadoop et Spark sont aujourd'hui les plus utilisés dans le monde de l'analyse de masse de données. Apache Lucene est le moteur d'indexation utilisé par tous les grands projets open-source comme par exemple les moteurs de recherche Apache Solr et ElasticSearch. Apache Kafka est un systène de queue de messages utilisé par plusieurs grandes entreprises comme Linkedin, Netflix ou Spotify.

Google

Google est actuellement le plus grand moteur de recherche du monde. Son business model est basé sur la publicité et les données utilisateurs. Cette particularité explique que Google soit la première entreprise à avoir rencontré des problèmes liés aux masses de données. La première contribution de Google a été faite sur l'analyse de masses de données de manière distribuée avec le paradigme de MapReduce. Ce type de programmation est expliqué dans un article de recherche de 2004 [MapReduce: Simplified Data Processing on Large Clusters](https://www.usenix.org/legacy/publications/library/proceedings/osdi04/tech/full_papers/dean/dean_html/index.html). Deux ans plus tard, Google va contribuer à nouveau sur les problèmatiques de masses de données en dévoilant BigTable, un système de stockage de données distribué à nouveau dans un article de recherche [Bigtable: A Distributed Storage System for Structured Data](http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/archive/bigtable-osdi06.pdf). Ces deux articles vont être la base des frameworks d'analyse de données Hadoop et Spark qui exploite le paradigme MapReduce. Les données sont stockées au format HDFS, un format dérivé de la solution BigTable propriétaire de Google.

Amazon

Amazon.com est une société de commerce électronique. En 2002, elle lance un nouveau produit appelé Amazon Web Services (AWS) permettant d'accèder à des fonctions latentes sur son site web. Ce nouveau produit va être un regroupement de services très utiles pour les entreprises intéressées dans le domaine de la masse de données. Tout d'abord avec Amazon Simple Storage Service (Amazon S3), un service de stockage de fichier volumineux. Il est possible de stocker très facilement des teraoctects de données dans S3 et de les analyser avec Hadoop ou Spark. Puis avec un système de serveurs virtualisé à la demande : Amazon Elastic Compute Cloud (Amazon EC2). Ce service permet de déploier des applications très facilement et de faire évoluer la puissance de calcul nécessaire en fonction du besoin en quelques clics. En 2013, Amazon dévoile Redshift, une base de données de type SQL permettant de traiter des masses de données efficacement. AWS permet aussi avec Amazon Elastic MapReduce (Amazon EMR) de traiter ses données via un cluster Hadoop ou Spark. Les prix très attractifs d'AWS peuvent expliquer l'engouement actuel pour ces technologies car de nombreuses entreprises ont pû se lancer dans des analyses sans investir des sommes astronomiques dans les machines et la configuration des clusters.

N.B. Voir pour le livre blanc

### A5

* Time behavior
* Resource utilization
* Stability

### A6

* Time behavior
    + Response time
    + Task time
    + ?

* Resource utilization
    + Transmission capacity
    + Number of memory related errors
    + ?

* Installability
    + Number of installation steps
    + Number of setup operations
    + ?

### A7



## Partie B


## Partie C
